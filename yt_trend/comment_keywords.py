from googleapiclient.discovery import build
from janome.tokenizer import Tokenizer
from collections import Counter
import time

# ğŸ”§ APIã‚­ãƒ¼ã¨ãƒãƒ£ãƒ³ãƒãƒ«IDï¼ˆã‚¹ãƒ†ã‚´ãƒ­ãƒ‘ãƒ³ãƒãƒ£ãƒ¼ã‚ºï¼‰
API_KEY = "AIzaSyApavwxqaBLkNyK65iHdgmUf9eX5CsYrmI"  # â† å¿…ãšã”è‡ªèº«ã®APIã‚­ãƒ¼ã«ç½®ãæ›ãˆã¦ãã ã•ã„
CHANNEL_ID = "UCusnpkgavQhPV_8e_zEh_jw"

# YouTube API åˆæœŸåŒ–
youtube = build("youtube", "v3", developerKey=API_KEY)

# âœ… æœ€æ–°ã®å‹•ç”»IDã‚’å–å¾—
def get_latest_video_id(channel_id):
    res = youtube.search().list(
        part="id",
        channelId=channel_id,
        order="date",
        maxResults=1,
        type="video"
    ).execute()
    return res["items"][0]["id"]["videoId"]

# âœ… ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†
def get_comments(video_id, max_results=100):
    comments = []
    next_page_token = None
    while len(comments) < max_results:
        res = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            textFormat="plainText",
            maxResults=100,
            pageToken=next_page_token
        ).execute()
        for item in res["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
        next_page_token = res.get("nextPageToken")
        if not next_page_token:
            break
        time.sleep(0.1)
    return comments[:max_results]

# âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
def extract_keywords(comments, top_n=30):
    tokenizer = Tokenizer()
    all_text = " ".join(comments)
    tokens = [token.surface for token in tokenizer.tokenize(all_text)
              if len(token.surface) > 1]  # 1æ–‡å­—èªã‚’é™¤å¤–
    counter = Counter(tokens)
    return [word for word, _ in counter.most_common(top_n)]

# âœ… å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
def main():
    print("ğŸ¬ æœ€æ–°å‹•ç”»ã‚’å–å¾—ä¸­...")
    latest_video_id = get_latest_video_id(CHANNEL_ID)
    print(f"ğŸ“º æœ€æ–°å‹•ç”»ID: {latest_video_id}")

    print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ä¸­...")
    comments = get_comments(latest_video_id, max_results=100)
    print(f"ğŸ’¾ ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {len(comments)}")

    print("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    keywords = extract_keywords(comments, top_n=30)

    print("ğŸ“ comment_keywords.txt ã«ä¿å­˜ä¸­...")
    with open("comment_keywords.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(keywords))

    print("âœ… å®Œäº†ï¼ã‚³ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
