from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import joblib

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®èª­ã¿è¾¼ã¿
X = pd.read_pickle("X.pkl")
y = pd.read_pickle("y.pkl")

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
model = XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)
model.fit(X_train, y_train)

# è©•ä¾¡
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"âœ… XGBoostå­¦ç¿’å®Œäº†ï¼RMSEï¼ˆlogã‚¹ã‚±ãƒ¼ãƒ«ï¼‰: {rmse:.4f}")

# ç‰¹å¾´é‡è¦åº¦ä¸Šä½10
importances = model.feature_importances_
top_features = pd.Series(importances, index=X.columns).sort_values(ascending=False).head(10)
print("\nğŸ“Š é‡è¦ãªç‰¹å¾´é‡TOP10ï¼š")
print(top_features)

# ä¿å­˜
joblib.dump(model, "xgb_model.pkl")
