{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825a354a-6cfb-447e-a7a8-d92bf9ee5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created package at: C:\\Users\\Owner\\youtube\\yt_trendlab\n"
     ]
    }
   ],
   "source": [
    "# ▶ パッケージ一式を作成（保存先をあなたの環境に合わせて変更OK）\n",
    "BASE_DIR = r\"C:\\Users\\Owner\\youtube\\yt_trendlab\"  # ←保存先\n",
    "import os, pathlib, textwrap\n",
    "\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "files = {}\n",
    "\n",
    "# __init__.py（エクスポート）\n",
    "files[\"__init__.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "from .pipeline import run_all\n",
    "from .thumbnail_features import THUMBNAIL_COLS, extract_all_thumbnail_features_mediapipe\n",
    "from .trending_utils import ensure_trending_snapshot_if_missing, add_trend_features\n",
    "from .text_features import tokenize_japanese, build_vectorizer\n",
    "from .modeling import train_rf, evaluate_rmse, feature_importance_df\n",
    "__all__ = [\n",
    "    \"run_all\",\n",
    "    \"THUMBNAIL_COLS\", \"extract_all_thumbnail_features_mediapipe\",\n",
    "    \"ensure_trending_snapshot_if_missing\", \"add_trend_features\",\n",
    "    \"tokenize_japanese\", \"build_vectorizer\",\n",
    "    \"train_rf\", \"evaluate_rmse\", \"feature_importance_df\",\n",
    "]\n",
    "'''\n",
    "\n",
    "# thumbnail_features.py（サムネ特徴）\n",
    "files[\"thumbnail_features.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import mediapipe as mp\n",
    "\n",
    "THUMBNAIL_COLS = [\n",
    "    \"brightness\",\"face_count\",\"telop_ratio\",\n",
    "    \"r_mean\",\"g_mean\",\"b_mean\",\"h_mean\",\"s_mean\",\"v_mean\"\n",
    "] + [f\"color_ratio_{i}\" for i in range(5)]\n",
    "\n",
    "def extract_all_thumbnail_features_mediapipe(url: str):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        hsv = cv2.cvtColor(arr, cv2.COLOR_RGB2HSV)\n",
    "        brightness = hsv[:,:,2].mean()\n",
    "        r_mean, g_mean, b_mean = arr[:,:,0].mean(), arr[:,:,1].mean(), arr[:,:,2].mean()\n",
    "        h_mean, s_mean, v_mean = hsv[:,:,0].mean(), hsv[:,:,1].mean(), hsv[:,:,2].mean()\n",
    "        pixels = arr.reshape(-1,3).astype(np.float32)\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "        _, labels, _ = cv2.kmeans(pixels, 5, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "        counts = np.bincount(labels.flatten(), minlength=5)\n",
    "        color_ratios = (counts / counts.sum()).tolist()\n",
    "        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "        telop_ratio = float((thresh==255).sum()) / float(thresh.size)\n",
    "        with mp.solutions.face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as fd:\n",
    "            res = fd.process(cv2.cvtColor(arr, cv2.COLOR_RGB2BGR))\n",
    "            face_count = len(res.detections) if res.detections else 0\n",
    "        return [brightness, face_count, telop_ratio,\n",
    "                r_mean, g_mean, b_mean, h_mean, s_mean, v_mean] + color_ratios\n",
    "    except Exception:\n",
    "        return [0]*len(THUMBNAIL_COLS)\n",
    "'''\n",
    "\n",
    "# trending_utils.py（急上昇）\n",
    "files[\"trending_utils.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, re, glob, pathlib, importlib.util\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "TREND_CSV_GLOB = \"trending_JP_*.csv\"\n",
    "TREND_USE_LAST_N = 14\n",
    "TREND_TOPK = 500\n",
    "TREND_VOCAB_PATH = \"trend_vocab.json\"\n",
    "\n",
    "# fetch_trending / trend_features のローダ\n",
    "def _load_trend_modules():\n",
    "    try:\n",
    "        from yt_trend.get_trending import fetch_trending as _fetch\n",
    "        from yt_trend.trend_features import (\n",
    "            build_trend_vocab_from_csvs as _build_vocab,\n",
    "            save_trend_vocab_json as _save_vocab,\n",
    "            load_trend_vocab_json as _load_vocab,\n",
    "            title_trend_features as _title_feats,\n",
    "        )\n",
    "        return _fetch, _build_vocab, _save_vocab, _load_vocab, _title_feats\n",
    "    except ModuleNotFoundError:\n",
    "        base = pathlib.Path.cwd() / \"code\"\n",
    "        gt, tf = base / \"get_trending.py\", base / \"trend_features.py\"\n",
    "        def _load(name, path):\n",
    "            spec = importlib.util.spec_from_file_location(name, str(path))\n",
    "            mod = importlib.util.module_from_spec(spec); spec.loader.exec_module(mod); return mod\n",
    "        if gt.exists() and tf.exists():\n",
    "            gt = _load(\"yt_trend_get_trending\", gt)\n",
    "            tf = _load(\"yt_trend_trend_features\", tf)\n",
    "            return gt.fetch_trending, tf.build_trend_vocab_from_csvs, tf.save_trend_vocab_json, tf.load_trend_vocab_json, tf.title_trend_features\n",
    "        raise\n",
    "\n",
    "_fetch, _build_vocab, _save_vocab, _load_vocab, _title_feats = _load_trend_modules()\n",
    "\n",
    "def ensure_trending_snapshot_if_missing(api_key_env=\"YT_API_KEY\", region=\"JP\", max_results=200):\n",
    "    csvs = glob.glob(TREND_CSV_GLOB)\n",
    "    if len(csvs)>0: return\n",
    "    api_key = os.getenv(api_key_env) or os.getenv(\"API_KEY\") or os.getenv(\"YOUTUBE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"⚠️ 急上昇CSVが無く、APIキー未設定のため自動取得をスキップ\"); return\n",
    "    print(\"ℹ️ 急上昇CSVが無いので、その場取得します…\")\n",
    "    df = _fetch(api_key, region_code=region, max_results=max_results)\n",
    "    out = f\"trending_JP_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Saved: {out} (rows={len(df)})\")\n",
    "\n",
    "def add_trend_features(titles: pd.Series) -> pd.DataFrame:\n",
    "    csvs = sorted(glob.glob(TREND_CSV_GLOB))\n",
    "    if len(csvs)==0:\n",
    "        return pd.DataFrame({\"trend_overlap_count\":[0]*len(titles),\n",
    "                             \"trend_overlap_ratio\":[0.0]*len(titles),\n",
    "                             \"trend_cosine_sim\":[0.0]*len(titles)})\n",
    "    def key(p):\n",
    "        m = re.search(r\"(\\d{8})\", os.path.basename(p)); return m.group(1) if m else \"00000000\"\n",
    "    csvs = sorted(csvs, key=key)[-TREND_USE_LAST_N:]\n",
    "    if os.path.exists(TREND_VOCAB_PATH):\n",
    "        hot = _load_vocab(TREND_VOCAB_PATH)\n",
    "        _, trend_titles = _build_vocab(csvs, top_k=TREND_TOPK)\n",
    "    else:\n",
    "        hot, trend_titles = _build_vocab(csvs, top_k=TREND_TOPK)\n",
    "        _save_vocab(hot, TREND_VOCAB_PATH)\n",
    "    feats = titles.fillna(\"\").apply(lambda t: _title_feats(t, hot, trend_titles_for_bow=trend_titles))\n",
    "    return pd.DataFrame(list(feats.values))\n",
    "'''\n",
    "\n",
    "# text_features.py（タイトルのTF-IDF）\n",
    "files[\"text_features.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "_tokenizer = Tokenizer()\n",
    "\n",
    "def tokenize_japanese(text: str):\n",
    "    return [t.base_form for t in _tokenizer.tokenize(text)\n",
    "            if t.part_of_speech.split(',')[0] in ['名詞','動詞','形容詞']]\n",
    "\n",
    "def build_vectorizer(max_features: int = 300):\n",
    "    return TfidfVectorizer(tokenizer=tokenize_japanese, token_pattern=None, max_features=max_features)\n",
    "'''\n",
    "\n",
    "# modeling.py（学習・評価）\n",
    "files[\"modeling.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_rf(X_train, y_train, n_estimators=100, max_depth=10, random_state=42, n_jobs=-1):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, n_jobs=n_jobs)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_rmse(model, X_test, y_test):\n",
    "    log_pred = model.predict(X_test)\n",
    "    y_pred  = np.expm1(log_pred).astype(int)\n",
    "    rmse_log = mean_squared_error(np.log1p(y_test), log_pred) ** 0.5\n",
    "    rmse_raw = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "    return rmse_log, rmse_raw, y_pred\n",
    "\n",
    "def feature_importance_df(model, columns, top=30):\n",
    "    imp = pd.Series(model.feature_importances_, index=columns)\n",
    "    return imp.sort_values(ascending=False).head(top)\n",
    "'''\n",
    "\n",
    "# pipeline.py（全部入り：読み込み→特徴量→学習→評価）\n",
    "files[\"pipeline.py\"] = r'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from isodate import parse_duration\n",
    "from .thumbnail_features import THUMBNAIL_COLS, extract_all_thumbnail_features_mediapipe\n",
    "from .trending_utils import ensure_trending_snapshot_if_missing, add_trend_features\n",
    "from .text_features import build_vectorizer\n",
    "from .modeling import train_rf, evaluate_rmse, feature_importance_df\n",
    "\n",
    "def _ensure_thumbnail_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if set(THUMBNAIL_COLS).issubset(df.columns): return df\n",
    "    if \"thumbnail\" not in df.columns:\n",
    "        raise ValueError(\"⚠️ 'thumbnail' 列がありません\")\n",
    "    feats = [extract_all_thumbnail_features_mediapipe(u) for u in df[\"thumbnail\"]]\n",
    "    return pd.concat([df.reset_index(drop=True),\n",
    "                      pd.DataFrame(feats, columns=THUMBNAIL_COLS)], axis=1)\n",
    "\n",
    "def _time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"weekday\"] = df[\"publishedAt\"].dt.weekday\n",
    "    df[\"hour\"] = df[\"publishedAt\"].dt.hour\n",
    "    df[\"is_weekend\"] = df[\"weekday\"].isin([5,6]).astype(int)\n",
    "    df[\"is_month_start\"] = df[\"publishedAt\"].dt.is_month_start.astype(int)\n",
    "    df[\"is_month_end\"] = df[\"publishedAt\"].dt.is_month_end.astype(int)\n",
    "    return df\n",
    "\n",
    "def run_all(xlsx_path: str, cutoff=\"2025-07-01\", tfidf_max_features=300):\n",
    "    # 1) load & basic clean\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"categoryId\"] = pd.to_numeric(df[\"categoryId\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    df[\"viewCount\"] = pd.to_numeric(df[\"viewCount\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"], utc=True)\n",
    "    df[\"duration_seconds\"] = df[\"duration\"].apply(lambda x: parse_duration(x).total_seconds() if pd.notnull(x) else 0)\n",
    "    df = df[df[\"duration_seconds\"] > 60].copy()\n",
    "\n",
    "    # 2) thumbnail\n",
    "    df = _ensure_thumbnail_features(df)\n",
    "\n",
    "    # 3) time\n",
    "    df = _time_features(df)\n",
    "\n",
    "    # 4) trending (CSVなければここで取得)\n",
    "    ensure_trending_snapshot_if_missing()\n",
    "    trend_df = add_trend_features(df[\"title\"])\n",
    "    df = pd.concat([df.reset_index(drop=True), trend_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # 5) TF-IDF\n",
    "    vectorizer = build_vectorizer(max_features=tfidf_max_features)\n",
    "    cutoff_ts = pd.to_datetime(cutoff, utc=True)\n",
    "    df_train = df[df[\"publishedAt\"] < cutoff_ts].copy()\n",
    "    df_test  = df[df[\"publishedAt\"] >= cutoff_ts].copy()\n",
    "\n",
    "    tfidf_train = vectorizer.fit_transform(df_train[\"title\"])\n",
    "    tfidf_test  = vectorizer.transform(df_test[\"title\"])\n",
    "    tfidf_cols  = [f\"tfidf_{w}\" for w in vectorizer.get_feature_names_out()]\n",
    "    tfidf_df_tr = pd.DataFrame(tfidf_train.toarray(), columns=tfidf_cols, index=df_train.index)\n",
    "    tfidf_df_te = pd.DataFrame(tfidf_test.toarray(),  columns=tfidf_cols, index=df_test.index)\n",
    "\n",
    "    # 6) assemble features\n",
    "    base_cols = [\"categoryId\",\"weekday\",\"hour\",\"is_weekend\",\"is_month_start\",\"is_month_end\"]\n",
    "    trend_cols = [\"trend_overlap_count\",\"trend_overlap_ratio\",\"trend_cosine_sim\"]\n",
    "    X_train = pd.concat([df_train[base_cols], df_train[[c for c in THUMBNAIL_COLS]], tfidf_df_tr], axis=1)\n",
    "    X_test  = pd.concat([df_test[base_cols],  df_test[[c for c in THUMBNAIL_COLS]],  tfidf_df_te], axis=1)\n",
    "    # trendは最後に concat（列順を固定しておく）\n",
    "    X_train = pd.concat([X_train, df_train[trend_cols]], axis=1).reset_index(drop=True)\n",
    "    X_test  = pd.concat([X_test,  df_test[trend_cols]],  axis=1).reset_index(drop=True)\n",
    "\n",
    "    y_train = np.log1p(df_train[\"viewCount\"])\n",
    "    y_test  = df_test[\"viewCount\"]\n",
    "\n",
    "    # 7) train & eval\n",
    "    model = train_rf(X_train, y_train)\n",
    "    rmse_log, rmse_raw, y_pred = evaluate_rmse(model, X_test, y_test)\n",
    "    imp_top = feature_importance_df(model, X_train.columns, top=30)\n",
    "\n",
    "    # 8) result table\n",
    "    df_result = df_test[[\"title\",\"publishedAt\",\"viewCount\"]].copy()\n",
    "    df_result[\"predicted_viewCount\"] = y_pred\n",
    "    df_result[\"abs_error\"] = (df_result[\"predicted_viewCount\"] - df_result[\"viewCount\"]).abs()\n",
    "    df_result = df_result.sort_values(\"publishedAt\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    metrics = {\"rmse_log\": rmse_log, \"rmse_raw\": rmse_raw}\n",
    "    return model, df_result, metrics, imp_top\n",
    "'''\n",
    "\n",
    "for name, content in files.items():\n",
    "    pathlib.Path(BASE_DIR, name).write_text(textwrap.dedent(content).strip()+\"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Created package at:\", BASE_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ca0d7-6d38-4cba-8e5b-b6679fed9014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (faceenv)",
   "language": "python",
   "name": "faceenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
